{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d834d6b8-13f5-4794-9c9e-56c907df0040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/horstl/git/Music-recognition\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0469fe4a-922f-4a46-acbd-acefaef3041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horstl/git/Music-recognition/venv/lib/python3.8/site-packages/torchaudio/backend/utils.py:46: UserWarning: \"torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE\" flag is deprecated and will be removed in 0.9.0. Please remove the use of flag.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from musicrecognition.train import get_song_paths\n",
    "from musicrecognition.audio_dataset import AudioDataset\n",
    "from musicrecognition.augmentation import get_augmenter\n",
    "from musicrecognition.spectrogram import get_spectrogram_func\n",
    "from musicrecognition.data_collate import create_collate_fn\n",
    "from musicrecognition.model import LSTMNetwork\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "BATCH_SIZE = 6\n",
    "TEST_SIZE = 0.2  # 70% train 30% test\n",
    "SONG_SAMPLE_RATE = 44100  # Most songs in the dataset seem to have a sample-rate of 44100\n",
    "MIN_AUDIO_LENGTH = 10\n",
    "MAX_AUDIO_LENGTH = 30\n",
    "LATENT_SPACE_SIZE = 48\n",
    "DATA_ROOT = Path('data')\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feba5bc8-1310-4022-90ae-12b1197442ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horstl/git/Music-recognition/venv/lib/python3.8/site-packages/torchaudio/functional/functional.py:357: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (256) may be set too high. Or, the value for `n_freqs` (1025) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "songs_paths = get_song_paths(DATA_ROOT / 'songs')\n",
    "random.shuffle(songs_paths)\n",
    "test_paths, train_paths = songs_paths[:round(len(songs_paths) * TEST_SIZE)], songs_paths[round(len(songs_paths) * TEST_SIZE):]\n",
    "\n",
    "test_set = AudioDataset(test_paths, SONG_SAMPLE_RATE)\n",
    "\n",
    "# Create augmentation function\n",
    "augmenter = get_augmenter(DATA_ROOT / 'background_noises')\n",
    "\n",
    "# Create spectrogram function\n",
    "spectrogram_func = get_spectrogram_func(SONG_SAMPLE_RATE)\n",
    "\n",
    "collate_fn = create_collate_fn(augmenter, MIN_AUDIO_LENGTH, MAX_AUDIO_LENGTH, SONG_SAMPLE_RATE, spectrogram_func)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=12, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2655ea2c-b194-4e71-9b0f-e84f12c3df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNetwork(256, 128, LATENT_SPACE_SIZE, 2)\n",
    "model.load_state_dict(torch.load('musicrecognition/model_430k_steps.pth'))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90411698-cd58-4844-a417-67f002321cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(test_loader)\n",
    "\n",
    "anchors, positives = next(iter_loader)\n",
    "anchors, positives = anchors.transpose(1, 2).to(device), positives.transpose(1, 2).to(device)\n",
    "\n",
    "latent_anchors = model(anchors).cpu()\n",
    "latent_positives = model(positives).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "756cf6b3-43a8-4856-a17b-64b73ea671c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 30.24498748779297\n",
      "negative: 31.706796646118164\n",
      "negative: 59.96929931640625\n",
      "negative: 79.36145782470703\n",
      "negative: 108.03630065917969\n",
      "negative: 177.42984008789062\n"
     ]
    }
   ],
   "source": [
    "anchors, positives = next(iter_loader)\n",
    "anchors, positives = anchors.transpose(1, 2).to(device), positives.transpose(1, 2).to(device)\n",
    "\n",
    "latent_anchors = model(anchors).cpu()\n",
    "latent_positives = model(positives).cpu()\n",
    "\n",
    "pos_id = 0\n",
    "latent_positive = latent_positives[pos_id]\n",
    "latent_anchor = latent_anchors[pos_id]\n",
    "print(f\"positive: {((latent_anchor-latent_positive)**2).sum(axis=0).item()}\")\n",
    "for i in range(len(latent_positives) - 1):\n",
    "    latent_negative = latent_positives[(i + pos_id + 1) % len(latent_positives)]\n",
    "    distance = ((latent_anchor-latent_negative)**2).sum(axis=0)\n",
    "    print(f\"negative: {distance.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cae288-0f5f-44a8-85d9-47e077cccd88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
